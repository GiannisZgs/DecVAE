{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86bc9b9",
   "metadata": {},
   "source": [
    "# DecVAE Tutorial: VOC_ALS Dataset\n",
    "\n",
    "Complete workflow example for the VOC_ALS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59684494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\Dell\\Files\\DecVAE\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the working directory to the DecVAE root\n",
    "# Adjust this path to your local DecVAE directory\n",
    "DECVAE_ROOT = Path(os.getcwd()).parent if 'examples' in os.getcwd() else Path(os.getcwd())\n",
    "os.chdir(DECVAE_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972b1c3",
   "metadata": {},
   "source": [
    "## 1. Prepare VOC_ALS Dataset\n",
    "\n",
    "The VOC_ALS dataset contains voice recordings from individuals with ALS (Amyotrophic Lateral Sclerosis) and healthy controls.\n",
    "\n",
    "Download the dataset from https://www.synapse.org/Synapse:syn53009474/wiki/624730 and place it in \"../VOC-ALS\" (same level as the DecVAE project directory).\n",
    "\n",
    "Then execute the data preparation script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992d76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added c:\\Users\\Dell\\Files\\DecVAE to Python path\n",
      "Reading metadata from ../VOC-ALS\\VOC-ALS.xlsx...\n",
      "Encoded ALSFRS-R_TotalScore into 8 intervals\n",
      "Encoded DiseaseDuration into 5 intervals\n",
      "Encoded KingClinicalStage into values 0-6\n",
      "Encoded Cantagallo_Questionnaire into 5 intervals\n",
      "Added phoneme encoding map (A,E,I,O,U,KA,PA,TA)\n",
      "Added category encoding map (HC,ALS)\n",
      "Added speaker_id encoding map for 153 speakers\n",
      "Succesfully encoded clinical variables\n",
      "Saved encoding maps to ./vocabularies\\voc_als_encodings.json\n",
      "Scanning directories for audio files...\n",
      "Found 1224 audio files across 153 subjects\n",
      "Subject distribution: 102 patients, 51 controls\n",
      "Organizing data by subject...\n",
      "  Part 1: 39 subjects\n",
      "Saving part 1 of processed data to ../VOC-ALS_preprocessed\\voc_als_data_part1.json.gz...\n",
      "Processed data saved to ../VOC-ALS_preprocessed\\voc_als_data_part1.json.gz\n",
      "  Part 2: 38 subjects\n",
      "Saving part 2 of processed data to ../VOC-ALS_preprocessed\\voc_als_data_part2.json.gz...\n",
      "Processed data saved to ../VOC-ALS_preprocessed\\voc_als_data_part2.json.gz\n",
      "  Part 3: 38 subjects\n",
      "Saving part 3 of processed data to ../VOC-ALS_preprocessed\\voc_als_data_part3.json.gz...\n",
      "Processed data saved to ../VOC-ALS_preprocessed\\voc_als_data_part3.json.gz\n",
      "  Part 4: 38 subjects\n",
      "Saving part 4 of processed data to ../VOC-ALS_preprocessed\\voc_als_data_part4.json.gz...\n",
      "Processed data saved to ../VOC-ALS_preprocessed\\voc_als_data_part4.json.gz\n",
      "Data saved to ../VOC-ALS_preprocessed\\voc_als_data.json.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/153 [00:00<?, ?it/s]\n",
      "  1%|          | 1/153 [00:01<04:12,  1.66s/it]\n",
      "  2%|▏         | 3/153 [00:01<01:10,  2.14it/s]\n",
      "  5%|▍         | 7/153 [00:01<00:25,  5.75it/s]\n",
      "  6%|▌         | 9/153 [00:02<00:19,  7.27it/s]\n",
      "  8%|▊         | 13/153 [00:02<00:12, 11.50it/s]\n",
      " 10%|█         | 16/153 [00:02<00:09, 13.86it/s]\n",
      " 12%|█▏        | 19/153 [00:02<00:08, 15.29it/s]\n",
      " 14%|█▍        | 22/153 [00:02<00:07, 16.98it/s]\n",
      " 16%|█▋        | 25/153 [00:02<00:06, 19.39it/s]\n",
      " 18%|█▊        | 28/153 [00:02<00:08, 15.00it/s]\n",
      " 20%|█▉        | 30/153 [00:03<00:08, 14.76it/s]\n",
      " 21%|██        | 32/153 [00:03<00:07, 15.72it/s]\n",
      " 23%|██▎       | 35/153 [00:03<00:06, 18.37it/s]\n",
      " 25%|██▍       | 38/153 [00:03<00:07, 16.12it/s]\n",
      " 27%|██▋       | 41/153 [00:03<00:06, 17.58it/s]\n",
      " 28%|██▊       | 43/153 [00:03<00:06, 17.61it/s]\n",
      " 29%|██▉       | 45/153 [00:04<00:08, 12.73it/s]\n",
      " 31%|███       | 47/153 [00:04<00:07, 14.05it/s]\n",
      " 32%|███▏      | 49/153 [00:04<00:07, 13.85it/s]\n",
      " 35%|███▍      | 53/153 [00:04<00:05, 18.21it/s]\n",
      " 37%|███▋      | 56/153 [00:04<00:05, 19.02it/s]\n",
      " 39%|███▊      | 59/153 [00:04<00:05, 18.62it/s]\n",
      " 40%|███▉      | 61/153 [00:04<00:05, 16.61it/s]\n",
      " 41%|████      | 63/153 [00:05<00:07, 12.01it/s]\n",
      " 43%|████▎     | 66/153 [00:05<00:06, 14.43it/s]\n",
      " 45%|████▌     | 69/153 [00:05<00:05, 16.64it/s]\n",
      " 47%|████▋     | 72/153 [00:05<00:04, 18.55it/s]\n",
      " 50%|████▉     | 76/153 [00:05<00:03, 22.48it/s]\n",
      " 52%|█████▏    | 79/153 [00:05<00:03, 20.57it/s]\n",
      " 54%|█████▎    | 82/153 [00:06<00:04, 15.59it/s]\n",
      " 55%|█████▍    | 84/153 [00:06<00:04, 15.44it/s]\n",
      " 57%|█████▋    | 87/153 [00:06<00:03, 16.74it/s]\n",
      " 58%|█████▊    | 89/153 [00:06<00:04, 15.29it/s]\n",
      " 59%|█████▉    | 91/153 [00:06<00:03, 15.63it/s]\n",
      " 61%|██████    | 93/153 [00:06<00:03, 16.43it/s]\n",
      " 63%|██████▎   | 96/153 [00:07<00:03, 17.03it/s]\n",
      " 65%|██████▍   | 99/153 [00:07<00:03, 15.01it/s]\n",
      " 67%|██████▋   | 103/153 [00:07<00:02, 18.66it/s]\n",
      " 69%|██████▉   | 106/153 [00:07<00:02, 18.33it/s]\n",
      " 71%|███████   | 109/153 [00:07<00:02, 19.28it/s]\n",
      " 73%|███████▎  | 112/153 [00:07<00:02, 18.46it/s]\n",
      " 75%|███████▍  | 114/153 [00:08<00:02, 18.31it/s]\n",
      " 76%|███████▌  | 116/153 [00:08<00:02, 18.15it/s]\n",
      " 77%|███████▋  | 118/153 [00:08<00:02, 12.91it/s]\n",
      " 78%|███████▊  | 120/153 [00:08<00:02, 13.36it/s]\n",
      " 80%|████████  | 123/153 [00:08<00:01, 15.25it/s]\n",
      " 82%|████████▏ | 126/153 [00:08<00:01, 17.17it/s]\n",
      " 84%|████████▎ | 128/153 [00:09<00:01, 15.07it/s]\n",
      " 86%|████████▌ | 131/153 [00:09<00:01, 17.60it/s]\n",
      " 88%|████████▊ | 134/153 [00:09<00:00, 19.15it/s]\n",
      " 90%|████████▉ | 137/153 [00:09<00:01, 12.06it/s]\n",
      " 92%|█████████▏| 140/153 [00:09<00:00, 13.99it/s]\n",
      " 93%|█████████▎| 142/153 [00:10<00:00, 12.41it/s]\n",
      " 94%|█████████▍| 144/153 [00:10<00:00, 13.49it/s]\n",
      " 96%|█████████▌| 147/153 [00:10<00:00, 15.41it/s]\n",
      " 97%|█████████▋| 149/153 [00:10<00:00, 13.95it/s]\n",
      " 99%|█████████▊| 151/153 [00:10<00:00, 15.02it/s]\n",
      "100%|██████████| 153/153 [00:11<00:00,  9.76it/s]\n",
      "100%|██████████| 153/153 [00:11<00:00, 13.85it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/misc/voc_als_prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229f3af",
   "metadata": {},
   "source": [
    "## 2. Input Visualization\n",
    "\n",
    "We generate input visualizations for the raw audio signal (X), and the components after applying a decomposition. We visualize individual components (OC1, OC2, ..., OCn) and aggregated representations, e.g. concatenation of all components and initial X [X,OC1,OC2,...,OCn]. We color the representations using frequency correspondence of the inputs or generative factors (phoneme, speaker, disease characteristics).\n",
    "\n",
    "For the VOC_ALS dataset, we will visualize the inputs to all models.\n",
    "\n",
    "Frame-level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frame-level inputs\n",
    "!accelerate launch scripts/visualize/low_dim_vis_input.py \\\n",
    "    --config_file config_files/input_visualizations/config_visualizing_input_frames_voc_als.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ddf06",
   "metadata": {},
   "source": [
    "Sequence-level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01188c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequence-level inputs\n",
    "!accelerate launch scripts/visualize/low_dim_vis_input.py \\\n",
    "    --config_file config_files/input_visualizations/config_visualizing_input_sequences_voc_als.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56421c0",
   "metadata": {},
   "source": [
    "## 3. Decompose the VOC-ALS dataset\n",
    "\n",
    "We will not pre-train a model for VOC-ALS, but instead use pre-trained on SimVowels or TIMIT models.\n",
    "\n",
    "We will still have to run the pre-training script to obtain the decomposed data. If the input_visualization has been generated earlier, then this step can be skipped.\n",
    "\n",
    "Single-GPU: use the --gpu_ids argument to specify the id of the GPU (0,1,2,...) - accelerate launch --gpu_ids <id> scripts... . Alternatively omit this argument and the default GPU id in your system will be used (as below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f767f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train DecVAE on single GPU\n",
    "!accelerate launch scripts/pre-training/base_models_ssl_pretraining.py \\\n",
    "    --config_file config_files/DecVAEs/voc_als/pre-training/config_pretraining_voc_als_NoC4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04026080",
   "metadata": {},
   "source": [
    "Multi-GPU (specify GPU IDs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876856ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train DecVAE on multiple GPUs (e.g., GPU 0 and 1)\n",
    "# Uncomment and modify as needed:\n",
    "# !accelerate launch --gpu_ids 0,1 scripts/pre-training/base_models_ssl_pretraining.py \\\n",
    "#     --config_file config_files/DecVAEs/voc_als/pre-training/config_pretraining_voc_als_NoC4.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695af4be",
   "metadata": {},
   "source": [
    "View configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89218293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config_files/DecVAEs/voc_als/pre-training/config_pretraining_voc_als_NoC4.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35ebd2",
   "metadata": {},
   "source": [
    "## 4. Latent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate latent representations\n",
    "!accelerate launch scripts/post-training/latents_post_analysis.py \\\n",
    "    --config_file config_files/DecVAEs/voc_als/latent_evaluations/config_latent_anal_voc_als.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff0dbb",
   "metadata": {},
   "source": [
    "## 5. Latent Visualization\n",
    "\n",
    "Frame-level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca629d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frame-level latent representations\n",
    "!accelerate launch scripts/visualize/low_dim_vis_latents.py \\\n",
    "    --config_file config_files/DecVAEs/voc_als/latent_visualizations/config_latent_frames_visualization_voc_als.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41224682",
   "metadata": {},
   "source": [
    "Sequence-level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a134725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequence-level latent representations\n",
    "!accelerate launch scripts/visualize/low_dim_vis_latents.py \\\n",
    "    --config_file config_files/DecVAEs/voc_als/latent_visualizations/config_latent_sequences_visualization_voc_als.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18d8d9",
   "metadata": {},
   "source": [
    "## 6. Latent Traversals\n",
    "\n",
    "Perform traversal analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de446dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform latent traversal analysis\n",
    "!accelerate launch scripts/latent_response_analysis/latent_traversal_analysis.py \\\n",
    "    --config_file config_files/DecVAEs/voc_als/latent_traversals/config_latent_traversals_voc_als.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DecSSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
